{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfb23ad5",
   "metadata": {},
   "source": [
    "# ETL Extraction Lab\n",
    "**Name**: Iranzi Innocent  \n",
    "**Student ID**: 67xxxx  \n",
    "\n",
    "This Jupyter Notebook hurt demonstrates Full and Incremental Extraction for an ETL process using a synthetic sales dataset (`custom_data.csv`). The dataset contains 100+ records of sales transactions. The notebook includes code to generate the dataset, perform extractions, and document each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd809b4",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "1. Install Python from [python.org](https://www.python.org).\n",
    "2. Install required libraries: `pip install pandas jupyter`\n",
    "3. Run Jupyter: `jupyter notebook`\n",
    "4. Open this notebook and run all cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a01b086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created and saved as 'custom_data.csv'\n",
      "   transaction_id        date  customer_id product  amount\n",
      "0               1  2025-03-23         1916   Phone   58.85\n",
      "1               2  2025-01-15         4752  Laptop  640.10\n",
      "2               3  2025-01-04         1525  Laptop  584.84\n",
      "3               4  2025-02-05         6168  Laptop  144.98\n",
      "4               5  2025-02-01         7572   Phone  560.75\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Generate 100 sales records\n",
    "data = {\n",
    "    'transaction_id': range(1, 101),\n",
    "    'date': [(datetime(2025, 1, 1) + timedelta(days=random.randint(0, 90))).strftime('%Y-%m-%d') for _ in range(100)],\n",
    "    'customer_id': [random.randint(1000, 9999) for _ in range(100)],\n",
    "    'product': [random.choice(['Laptop', 'Phone', 'Tablet', 'Headphones']) for _ in range(100)],\n",
    "    'amount': [round(random.uniform(50, 1000), 2) for _ in range(100)]\n",
    "}\n",
    "\n",
    "# Create DataFrame and save to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('custom_data.csv', index=False)\n",
    "print(\"Dataset created and saved as 'custom_data.csv'\")\n",
    "print(df.head())  # Display first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc7498b",
   "metadata": {},
   "source": [
    "# Full Extraction\n",
    "This section performs a full extraction by reading all records from `custom_data.csv` into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df7a2c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Extraction: Loaded all records\n",
      "   transaction_id        date  customer_id product  amount\n",
      "0               1  2025-03-23         1916   Phone   58.85\n",
      "1               2  2025-01-15         4752  Laptop  640.10\n",
      "2               3  2025-01-04         1525  Laptop  584.84\n",
      "3               4  2025-02-05         6168  Laptop  144.98\n",
      "4               5  2025-02-01         7572   Phone  560.75\n",
      "Total records extracted: 100\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Perform Full Extraction\n",
    "full_data = pd.read_csv('custom_data.csv')\n",
    "print(\"Full Extraction: Loaded all records\")\n",
    "print(full_data.head())  # Display first 5 rows\n",
    "print(f\"Total records extracted: {len(full_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b2a5ab",
   "metadata": {},
   "source": [
    "# Incremental Extraction\n",
    "This section extracts records with a `date` later than the timestamp stored in `last_extraction.txt`. If the file is empty or missing, it uses a default old date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e748b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incremental Extraction: Loaded records after 2025-04-05 00:00:00\n",
      "Empty DataFrame\n",
      "Columns: [transaction_id, date, customer_id, product, amount]\n",
      "Index: []\n",
      "Records extracted: 0\n",
      "No new records to update last_extraction.txt\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Read the last extraction timestamp\n",
    "try:\n",
    "    with open('last_extraction.txt', 'r') as file:\n",
    "        last_extraction = file.read().strip()\n",
    "        last_extraction_date = datetime.strptime(last_extraction, '%Y-%m-%d')\n",
    "except (FileNotFoundError, ValueError):\n",
    "    # Use a default old date if file is empty or missing\n",
    "    last_extraction_date = datetime(2022, 1, 1)\n",
    "\n",
    "# Perform Incremental Extraction\n",
    "full_data['date'] = pd.to_datetime(full_data['date'])  # Convert date column to datetime\n",
    "incremental_data = full_data[full_data['date'] > last_extraction_date]\n",
    "print(\"Incremental Extraction: Loaded records after\", last_extraction_date)\n",
    "print(incremental_data.head())\n",
    "print(f\"Records extracted: {len(incremental_data)}\")\n",
    "\n",
    "# Update last_extraction.txt with the latest date\n",
    "if not incremental_data.empty:\n",
    "    latest_date = incremental_data['date'].max().strftime('%Y-%m-%d')\n",
    "    with open('last_extraction.txt', 'w') as file:\n",
    "        file.write(latest_date)\n",
    "    print(f\"Updated last_extraction.txt with {latest_date}\")\n",
    "else:\n",
    "    print(\"No new records to update last_extraction.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da447485",
   "metadata": {},
   "source": [
    "# Testing Incremental Extraction\n",
    "This section adds new records to `custom_data.csv` with later dates and re-runs the incremental extraction to demonstrate it extracts only the new records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "991c13a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 5 new records to custom_data.csv\n",
      "   transaction_id        date  customer_id     product  amount\n",
      "0             101  2025-04-01         1452      Laptop  464.57\n",
      "1             102  2025-04-02         2889      Laptop  985.02\n",
      "2             103  2025-04-03         5279  Headphones  159.86\n",
      "3             104  2025-04-04         3925      Tablet  904.53\n",
      "4             105  2025-04-05         5349      Tablet  230.57\n"
     ]
    }
   ],
   "source": [
    "# Simulate new data\n",
    "new_data = {\n",
    "    'transaction_id': range(101, 106),\n",
    "    'date': [(datetime(2025, 4, 1) + timedelta(days=i)).strftime('%Y-%m-%d') for i in range(5)],\n",
    "    'customer_id': [random.randint(1000, 9999) for _ in range(5)],\n",
    "    'product': [random.choice(['Laptop', 'Phone', 'Tablet', 'Headphones']) for _ in range(5)],\n",
    "    'amount': [round(random.uniform(50, 1000), 2) for _ in range(5)]\n",
    "}\n",
    "new_df = pd.DataFrame(new_data)\n",
    "new_df.to_csv('custom_data.csv', mode='a', header=False, index=False)\n",
    "print(\"Added 5 new records to custom_data.csv\")\n",
    "print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f03e9068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incremental Extraction: Loaded records after 2025-04-05 00:00:00\n",
      "Empty DataFrame\n",
      "Columns: [transaction_id, date, customer_id, product, amount]\n",
      "Index: []\n",
      "Records extracted: 0\n",
      "No new records to update last_extraction.txt\n"
     ]
    }
   ],
   "source": [
    "# Reload the updated dataset\n",
    "full_data = pd.read_csv('custom_data.csv')\n",
    "\n",
    "# Re-run Incremental Extraction\n",
    "try:\n",
    "    with open('last_extraction.txt', 'r') as file:\n",
    "        last_extraction = file.read().strip()\n",
    "        last_extraction_date = datetime.strptime(last_extraction, '%Y-%m-%d')\n",
    "except (FileNotFoundError, ValueError):\n",
    "    last_extraction_date = datetime(2020, 1, 1)\n",
    "\n",
    "full_data['date'] = pd.to_datetime(full_data['date'])\n",
    "incremental_data = full_data[full_data['date'] > last_extraction_date]\n",
    "print(\"Incremental Extraction: Loaded records after\", last_extraction_date)\n",
    "print(incremental_data)\n",
    "print(f\"Records extracted: {len(incremental_data)}\")\n",
    "\n",
    "if not incremental_data.empty:\n",
    "    latest_date = incremental_data['date'].max().strftime('%Y-%m-%d')\n",
    "    with open('last_extraction.txt', 'w') as file:\n",
    "        file.write(latest_date)\n",
    "    print(f\"Updated last_extraction.txt with {latest_date}\")\n",
    "else:\n",
    "    print(\"No new records to update last_extraction.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8894380",
   "metadata": {},
   "source": [
    "## Transform Full Data\n",
    "This section applies transformations to the full dataset and saves the result to `transformed_full.csv`. The transformations include:\n",
    "- **Cleaning**: Remove duplicates based on `transaction_id` and handle missing values.\n",
    "- **Enrichment**: Add a `product_category` column to group products.\n",
    "- **Structural**: Ensure `date` is in datetime format and add a `month` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6242fe11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: Removed duplicates and handled missing values\n",
      "Records after cleaning: 105\n",
      "Enrichment: Added product_category column\n",
      "Structural: Standardized date and added month column\n",
      "Saved transformed full data to 'transformed_full.csv'\n",
      "   transaction_id       date  customer_id product  amount product_category  \\\n",
      "0               1 2025-03-23         1916   Phone   58.85           Mobile   \n",
      "1               2 2025-01-15         4752  Laptop  640.10        Computing   \n",
      "2               3 2025-01-04         1525  Laptop  584.84        Computing   \n",
      "3               4 2025-02-05         6168  Laptop  144.98        Computing   \n",
      "4               5 2025-02-01         7572   Phone  560.75           Mobile   \n",
      "\n",
      "     month  \n",
      "0  2025-03  \n",
      "1  2025-01  \n",
      "2  2025-01  \n",
      "3  2025-02  \n",
      "4  2025-02  \n"
     ]
    }
   ],
   "source": [
    "# Load full dataset\n",
    "full_data = pd.read_csv('custom_data.csv')\n",
    "\n",
    "# Transformation 1: Cleaning\n",
    "# Remove duplicates based on transaction_id\n",
    "full_data = full_data.drop_duplicates(subset=['transaction_id'], keep='first')\n",
    "# Handle missing values\n",
    "if full_data.isnull().sum().any():\n",
    "    full_data['amount'] = full_data['amount'].fillna(full_data['amount'].median())\n",
    "    full_data['product'] = full_data['product'].fillna(full_data['product'].mode()[0])\n",
    "    full_data['customer_id'] = full_data['customer_id'].fillna(full_data['customer_id'].mode()[0])\n",
    "print(\"Cleaning: Removed duplicates and handled missing values\")\n",
    "print(f\"Records after cleaning: {len(full_data)}\")\n",
    "\n",
    "# Transformation 2: Enrichment\n",
    "# Add product_category column\n",
    "product_category_map = {\n",
    "    'Laptop': 'Computing',\n",
    "    'Tablet': 'Computing',\n",
    "    'Phone': 'Mobile',\n",
    "    'Headphones': 'Accessories'\n",
    "}\n",
    "full_data['product_category'] = full_data['product'].map(product_category_map)\n",
    "print(\"Enrichment: Added product_category column\")\n",
    "\n",
    "# Transformation 3: Structural\n",
    "# Ensure date is datetime and add month column\n",
    "full_data['date'] = pd.to_datetime(full_data['date'])\n",
    "full_data['month'] = full_data['date'].dt.strftime('%Y-%m')\n",
    "print(\"Structural: Standardized date and added month column\")\n",
    "\n",
    "# Save transformed data\n",
    "full_data.to_csv('transformed_full.csv', index=False)\n",
    "print(\"Saved transformed full data to 'transformed_full.csv'\")\n",
    "print(full_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2187ab79",
   "metadata": {},
   "source": [
    "## Transform Incremental Data\n",
    "This section applies the same transformations to the incremental dataset and saves the result to `transformed_incremental.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9476e2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: Removed duplicates and handled missing values\n",
      "Records after cleaning: 0\n",
      "Enrichment: Added product_category column\n",
      "Structural: Standardized date and added month column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved transformed incremental data to 'transformed_incremental.csv'\n",
      "Empty DataFrame\n",
      "Columns: [transaction_id, date, customer_id, product, amount, product_category, month]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Use the incremental data from the last extraction\n",
    "# Apply the same transformations\n",
    "\n",
    "# Transformation 1: Cleaning\n",
    "incremental_data = incremental_data.drop_duplicates(subset=['transaction_id'], keep='first')\n",
    "if incremental_data.isnull().sum().any():\n",
    "    incremental_data['amount'] = incremental_data['amount'].fillna(incremental_data['amount'].median())\n",
    "    incremental_data['product'] = incremental_data['product'].fillna(incremental_data['product'].mode()[0])\n",
    "    incremental_data['customer_id'] = incremental_data['customer_id'].fillna(incremental_data['customer_id'].mode()[0])\n",
    "print(\"Cleaning: Removed duplicates and handled missing values\")\n",
    "print(f\"Records after cleaning: {len(incremental_data)}\")\n",
    "\n",
    "# Transformation 2: Enrichment\n",
    "incremental_data['product_category'] = incremental_data['product'].map(product_category_map)\n",
    "print(\"Enrichment: Added product_category column\")\n",
    "\n",
    "# Transformation 3: Structural\n",
    "incremental_data['date'] = pd.to_datetime(incremental_data['date'])\n",
    "incremental_data['month'] = incremental_data['date'].dt.strftime('%Y-%m')\n",
    "print(\"Structural: Standardized date and added month column\")\n",
    "\n",
    "# Save transformed data\n",
    "incremental_data.to_csv('transformed_incremental.csv', index=False)\n",
    "print(\"Saved transformed incremental data to 'transformed_incremental.csv'\")\n",
    "print(incremental_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f5fa47",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook successfully demonstrates:\n",
    "- Full Extraction: Loading all 100+ records from `custom_data.csv`.\n",
    "- Incremental Extraction: Extracting only new records based on the timestamp in `last_extraction.txt`.\n",
    "- Transformations: Applying cleaning, enrichment, and structural changes to both full and incremental datasets.\n",
    "- Testing: Adding new records and verifying incremental extraction and transformation.\n",
    "The dataset, transformed outputs, and code are included in the repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
